{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF4zo1LFJ7X8"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>فاز اول پروژه</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ۸ فروردین <br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SeSNtgoJ7X_"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=6>\n",
    "    <h1>\n",
    "    <b>مقدمه</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "    پروژه‌ی درس متشکل از ۳ فاز است.\n",
    "    دادگان مورد استفاده در پروژه، مقالات علمی استخراج شده از سایت <a href=\"https://www.semanticscholar.org/\">Semantic Scholar</a> هستند. مقالات به دو دسته‌ی \n",
    "    «هوش مصنوعی , بیوانفورماتیک» و «سخت‌افزار و سیستم» \n",
    "    تقسیم شده‌اند. تخصصی بودن حوزه‌ی بازیابی می‌تواند به بهبود کیفیت آن کمک نماید.\n",
    "    <b><u>\n",
    "    پیش از هر چیز ابتدا یکی از این دو دسته را برای کار انتخاب نمایید.\n",
    "    </u></b>\n",
    "     امیدواریم تا انتهای پروژه بتوانید برای یکی از این دسته‌ها یک سیستم جست‌و‌جوی مقالات بسیار خوب پیاده‌سازی کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqIX14fXJ7YA"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h1>\n",
    "    <b>فاز اول</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "    هدف از فاز اول پروژه، طراحی و پیاده سازی سیستم بازیابی اطلاعات برای مجموعه دادگان ارائه شده است. در این فاز شناسه، چکیده و عنوان مقالات در اختیارتان قرار گرفته است.\n",
    "    در ابتدای هر بخش توضیح کوتاهی درباره آن بخش آورده شده است. در تمامی کد نیاز است که قسمت‌های TODO را کامل کنید. ملاک نمره دهی در این فاز صحت عملکرد توابع سیستم است. بنابراین از اجرا شدن کد خود اطمینان حاصل کنید و هر جا نیاز به توضیح بود می‌توانید به صورت کامنت یا در ادامه کد توضیح مربوطه خود را بنویسید (ارائه توضیح ضروری نیست و تنها در صورتی که شما احساس نیاز کردید، می‌توانید توضیح کوتاهی ارائه کنید. بنابراین ارائه توضیح نمره نخواهد داشت.) در زمان آپلود فراموش نکنید هر فایلی که نیاز است را آپلود کنید. نمره ی کامل این فاز ۱۰۰ بوده و مابقی امتیازیست.\n",
    "    <br>\n",
    "     تنها زبان قابل قبول برای پروژه پایتون است. محدودیت استفاده از کتاب‌خانه‌های آماده در هر بخش مشخص شده است. در انتهای پروژه قرار است یک سیستم یکپارچه‌ی جست‌و‌جو داشته باشید، بنابراین به پیاده‌سازی هر چه بهتر این فاز توجه داشته باشید.\n",
    "</font>\n",
    "</div>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACOS-cfjJ7YB"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پیش‌پردازش و آماده‌سازی داده‌ها (۶ + ۲ + ۱ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "    در بخش اول ابتدا داده‌ها را از فایل خوانده و سپس با استفاده از کتابخانه‌های آماده‌ به پیش پردازش آنها بپردازید. در این قسمت نحوه پیاده سازی به شما برمی‌گردد. کتابخانه‌هایی که می‌توانید از آنها استفاده کنید <a href=\"https://spacy.io/\">SpaCy</a>  و <a href=\"https://www.nltk.org/\">NLTK</a>  است. \n",
    "    در این قسمت تابع clean_data() را پیاده سازی کنید. عملکرد تابع به این صورت است که یک متن به عنوان ورودی گرفته و توکن‌های ولید آن را به صورت یک لیست که عملیات lemmatization, stemming و case folding اجرا شده است خروجی می‌دهد. متن ورودی شامل عنوان مقاله یا چکیده آنهاست. \n",
    "    دقت کنید علائم نگارشی باید از متون حذف شده باشند. در قسمت بعد کلمات اضافه بی تاثیر را یافته و مطابق توضیحات عمل کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RowHVRtLAB7",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import re, string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag, PorterStemmer\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gb--m01QME3k",
    "outputId": "568081e9-0bfd-4624-fec9-3cb1ccecbc45",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "id": "juaWjmg6KrBz"
   },
   "outputs": [],
   "source": [
    "pos_map = {\n",
    "    'JJ': wordnet.ADJ,\n",
    "    'VB': wordnet.VERB,\n",
    "    'RB': wordnet.ADV,\n",
    "    'NN': wordnet.NOUN,\n",
    "    'DT': wordnet.NOUN,\n",
    "    'VBZ': wordnet.VERB,\n",
    "    'IN': wordnet.NOUN\n",
    "}\n",
    "\n",
    "def pos_to_wordnet(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YKR9hRghLkcw",
    "outputId": "9b5e284f-4b8a-49fc-f4fd-39deb0def184"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('an', 'DT'),\n",
       " ('abstract', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('better', 'JJR'),\n",
       " ('summary', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('main', 'JJ'),\n",
       " ('article', 'NN'),\n",
       " ('going', 'VBG'),\n",
       " ('bags', 'NNS')]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(['an', 'abstract', 'is', 'a', 'better', 'summary', 'of', 'the', 'main', 'article', 'going', 'bags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "id": "CrcnnWkKJ7YB"
   },
   "outputs": [],
   "source": [
    "## 6 points\n",
    "def pre_clean(text):\n",
    "    bad_chars = ['“', '”']\n",
    "\n",
    "    for t in bad_chars:\n",
    "        text=text.replace(t,\" \")\n",
    "    \n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = regex.sub(' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.casefold()\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def clean_data(text : str):\n",
    "    \"\"\"Preprocesses the text with tokenization, case folding, stemming and lemmatization, and punctuations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The title or abstract of an article\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of tokens\n",
    "    \"\"\"\n",
    "    tokens = pre_clean(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = list(map(lambda token: lemmatizer.lemmatize(token, pos=pos_to_wordnet(pos_tag([token])[0][1])), tokens))\n",
    "\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3KcRmFNuUKu",
    "outputId": "0897cd8a-2074-4310-d651-9a4a9e90d536"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an',\n",
       " 'abstract',\n",
       " 'be',\n",
       " 'a',\n",
       " 'well',\n",
       " 'summary',\n",
       " 'of',\n",
       " 'the',\n",
       " 'main',\n",
       " 'article',\n",
       " 'go',\n",
       " 'bag']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = \"An abstract is a  better summary of the main article going bags.\"\n",
    "clean_data(t) # return [\"an\", \"abstract\", \"is\", \"a\", \"summary\", \"of\", \"the\", \"main\", \"article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bT7bDlFHubxs",
    "outputId": "390b3bdf-6961-4f52-ff75-5aca1fa47b5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an',\n",
       " 'abstract',\n",
       " 'is',\n",
       " 'a',\n",
       " 'better',\n",
       " 'summary',\n",
       " 'of',\n",
       " 'the',\n",
       " 'main',\n",
       " 'article',\n",
       " 'going',\n",
       " 'bags']"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_clean(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRyJujgbJ7YD"
   },
   "source": [
    "<p></p>\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در مرحله بعد تابع find_stop_words() را پیاده سازی کنید که کارکرد آن پیدا کردن توکن‌های اضافه است. \n",
    "توجه کنید که برای حذف stop wordها باید به گونه ای عمل کنید که ابتدا توکن‌های با بیشترین تکرار را  پیدا کرده و سپس به ۳۰ توکن با بیشترین تکرار را از میان تمامی توکن‌های متن حذف کنید. بدیهی است که stop word ها معمولا معنای زیادی همراه با خود ندارند. به این نکته در حذف آنها توجه کنید. در نهایت stop word هایی را که یافته‌اید به همراه تعداد دفعات تکرار به هر فرمتی در خروجی چاپ کنید. (۲ نمره)\n",
    "</font>\n",
    "</div>\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "994aokTgJ7YD",
    "outputId": "16534221-68d6-4351-95cd-cd84522f768f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 3),\n",
       " ('and', 3),\n",
       " ('the', 3),\n",
       " ('TODO', 1),\n",
       " (':', 1),\n",
       " ('read', 1),\n",
       " ('file', 1),\n",
       " ('do', 1),\n",
       " ('preprocessing', 1),\n",
       " ('on', 1),\n",
       " ('(', 1),\n",
       " ('first', 1),\n",
       " ('clean', 1),\n",
       " ('then', 1),\n",
       " ('detect', 1),\n",
       " ('delete', 1),\n",
       " ('stop', 1),\n",
       " ('words', 1),\n",
       " (')', 1)]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2 points\n",
    "\n",
    "\n",
    "def find_stop_words(all_text, num_token=30):\n",
    "    \"\"\"Detects stop-words\n",
    "\n",
    "     Parameters\n",
    "    ----------\n",
    "    all_text : list of all tokens\n",
    "        (result of clean_data(text) for all the text)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Return Value is optional but must print the stop words and number of their occurence\n",
    "    \"\"\" \n",
    "\n",
    "    text = (' ').join(all_text)\n",
    "\n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "    dist = nltk.FreqDist(words)\n",
    "    stop_words = dist.most_common(num_token)\n",
    "    return stop_words\n",
    "    \n",
    "\n",
    "find_stop_words(['TODO: read data file and do the preprocessing on the data (first clean the data and then detect and delete stop words)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIWVnrceJ7YD",
    "outputId": "9cc5eb07-cfc0-42bc-c993-5f22f72bddc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 41433), ('of', 28762), ('and', 26242), ('be', 21354), ('a', 18393), ('in', 17634), ('to', 17187), ('for', 8191), ('with', 7501), ('that', 6528), ('we', 6260), ('on', 5280), ('this', 4729), ('by', 4512), ('use', 4120), ('have', 4022), ('model', 3712), ('an', 3207), ('cell', 3190), ('from', 3173), ('method', 3077), ('data', 2983), ('it', 2563), ('which', 2456), ('result', 2431), ('can', 2397), ('image', 2378), ('base', 2342), ('study', 2310), ('or', 2232)]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/data.csv')\n",
    "all_text = list(df[df['anstract'].notnull()]['anstract'])\n",
    "all_text = list(map(clean_data, all_text))\n",
    "stop_words = find_stop_words([' '.join(item) for item in all_text])\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tmV4HcUJS25",
    "outputId": "4d8479c1-1bcb-4793-ffa4-39de52a2fcae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6185, 5)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_list = [x[0] for x in stop_words]\n",
    "df = df.fillna(-1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBwr88TKH3Dm",
    "outputId": "60fe6e14-e653-4377-8d4c-2b7faef8a7eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 38s, sys: 5.01 s, total: 1min 43s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_dict = {}\n",
    "paper_ids = list(df['paperId'])\n",
    "paper_id_map = {}\n",
    "max_id = 1\n",
    "for paper_id in paper_ids:\n",
    "  title = df[df['paperId']==paper_id].iloc[0]['title']\n",
    "  abstract = df[df['paperId']==paper_id].iloc[0]['anstract']\n",
    "  doc_dict[max_id] = {\n",
    "      'title':[t for t in clean_data(title) if t not in stop_words_list],\n",
    "      'abstract':[t for t in clean_data(abstract) if t not in stop_words_list] if abstract != -1 else None\n",
    "  }\n",
    "  paper_id_map[paper_id] = max_id\n",
    "  max_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "id": "7pi1kOcDTMt9"
   },
   "outputs": [],
   "source": [
    "def add_doc_to_doc_dict(doc_id, title, abstract):\n",
    "  global max_id\n",
    "  if doc_id not in paper_id_map.keys():\n",
    "    doc_dict[max_id] = {\n",
    "      'title':[t for t in clean_data(title) if t not in stop_words_list],\n",
    "      'abstract':[t for t in clean_data(abstract) if t not in stop_words_list] if abstract != -1 else None\n",
    "    }\n",
    "    paper_id_map[doc_id] = max_id\n",
    "    max_id += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voI8R2L6J7YE"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>نمایه‌سازی (۱۲ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     در این بخش باید برای سامانه positional index بسازید.<br>\n",
    "    با توجه به مواردی که در بخش بعد می‌آید و نیاز به جست‌وجو‌ی مجزا و با امتیازدهی متفاوت بر روی بخش‌های مختلف سند مثل عنوان یا چکیده آن، در این قسمت باید نمایه‌ی مناسب برای امکان جست‌وجو‌ در بخش‌های مختلف پیاده‌سازی کنید. با استفاده از نمایه‌ی ساخته‌شده باید بتوان شماره تمامی اسنادی که یک کلمه در آن آمده است و همچنین همه جایگاه‌های این کلمه در هر بخش هر سند را پیدا کرد.<br>\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "id": "mgG9ktdASKHr"
   },
   "outputs": [],
   "source": [
    "def add_doc_to_index(i, posting_lists):\n",
    "  if doc_dict[i]['title']:\n",
    "    for j, token in enumerate(doc_dict[i]['title']):\n",
    "      if token in posting_lists.keys():\n",
    "        if i in posting_lists[token]['title'].keys():\n",
    "          posting_lists[token]['title'][i].append(j+1)\n",
    "        else:\n",
    "          posting_lists[token]['title'][i] = [j+1]\n",
    "      else:\n",
    "        posting_lists[token] = {\n",
    "            'title': {i: [j+1]},\n",
    "            'abstract': {}\n",
    "        }\n",
    "  if doc_dict[i]['abstract']:\n",
    "    for j, token in enumerate(doc_dict[i]['abstract']):\n",
    "      if token in posting_lists.keys():\n",
    "        if i in posting_lists[token]['abstract'].keys():\n",
    "          posting_lists[token]['abstract'][i].append(j+1)\n",
    "        else:\n",
    "          posting_lists[token]['abstract'][i] = [j+1]\n",
    "\n",
    "      else:\n",
    "        posting_lists[token] = {\n",
    "            'title': {},\n",
    "            'abstract': {i: [j+1]}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "id": "VGASPsf-J7YE"
   },
   "outputs": [],
   "source": [
    "## 12 points\n",
    "\n",
    "def construct_positional_indexes(corpus : dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    Get processed data and insert words in that into a trie and construct postional_index and posting lists after wards.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: str\n",
    "        processed data \n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    docs: \n",
    "        list of docs with specificied id, title, abstract.\n",
    "    \"\"\"\n",
    "    posting_lists = {}\n",
    "    for paper_id in paper_id_map.keys():\n",
    "      add_doc_to_index(paper_id_map[paper_id], posting_lists)\n",
    "    return posting_lists\n",
    "\n",
    "\n",
    "docs = construct_positional_indexes(doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "id": "eM20lS35TWV3"
   },
   "outputs": [],
   "source": [
    "_end = '_end_'\n",
    "def make_trie(words):\n",
    "     root = dict()\n",
    "     for word in words:\n",
    "         current_dict = root\n",
    "         for letter in word:\n",
    "             current_dict = current_dict.setdefault(letter, {})\n",
    "         current_dict[_end] = _end\n",
    "     return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "IuO89U1eTvFj"
   },
   "outputs": [],
   "source": [
    "def in_trie(trie, word):\n",
    "     current_dict = trie\n",
    "     for letter in word:\n",
    "         if letter not in current_dict:\n",
    "             return False\n",
    "         current_dict = current_dict[letter]\n",
    "     return _end in current_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Amvs8D_WUR5O",
    "outputId": "761125bd-9ee7-4beb-8d3a-f4d9583dbfe4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['analysis',\n",
       " 'ground',\n",
       " 'surface',\n",
       " 'ultrasonic',\n",
       " 'face',\n",
       " 'grind',\n",
       " 'silicon',\n",
       " 'carbide',\n",
       " 'sic',\n",
       " 'ceramic']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(docs.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "4AycWRGcTnpl"
   },
   "outputs": [],
   "source": [
    "trie = make_trie(list(docs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0tgD-BmLTsG2",
    "outputId": "4dacde9b-b076-42b6-e25c-4c41f889790c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_trie(trie, 'ceramic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JmlNqDtlT7fe",
    "outputId": "ca5849e8-a151-4185-ffeb-5414c3c3bae9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_trie(trie, 'aadmjoweif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss5_1CloJ7YF"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>مشاهده (۱ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "        این بخش برای مشاهده \n",
    "        posting list\n",
    "        یک کلمه و جایگاه‌های کلمه در هر بخش سند است.<br>\n",
    "         تابع\n",
    "        get_posting_list\n",
    "        با گرفتن\n",
    "        word\n",
    "        به عنوان کلمه ورودی، یک دیکشنری به عنوان خروجی بر می‌گرداند که کلید‌های دیکشنری شناسه سند‌هایی است که کلمه در آن وجود داشته‌است.\n",
    "            برای هر شناسه سند آمده در کلید‌های دیکشنری، یک دیکشنری به عنوان مقدار وجود خواهد داشت که کلید‌های آن می‌تواند \n",
    "        title\n",
    "        و\n",
    "        abstract\n",
    "        باشد که جایگاه‌های آمدن کلمه در بخش‌های عنوان و چکیده به صورت لیست به عنوان مقدار هر یک از این کلید‌ها می‌آید. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pkBXIIDyJ7YF",
    "outputId": "15ef0b1f-4404-495c-bcf9-9246188a811b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ceramic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': {1: [10],\n",
       "  2181: [3],\n",
       "  2361: [10],\n",
       "  2496: [10],\n",
       "  2625: [6],\n",
       "  2905: [13],\n",
       "  3237: [7],\n",
       "  3330: [9],\n",
       "  3822: [10],\n",
       "  4838: [5]},\n",
       " 'abstract': {70: [31, 40],\n",
       "  863: [4, 77],\n",
       "  965: [138],\n",
       "  2496: [59, 77, 128],\n",
       "  2903: [130],\n",
       "  3580: [82],\n",
       "  3822: [4],\n",
       "  4838: [4, 44]}}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1 points\n",
    "\n",
    "def get_posting_list(word : str, docs):\n",
    "    \n",
    "    \"\"\" get posting_list of a word\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        word: str\n",
    "             word we want to check\n",
    "\n",
    "        Return\n",
    "        ----------\n",
    "        dict \n",
    "            posting list\n",
    "\n",
    "    \"\"\"\n",
    "    word = ' '.join(clean_data(word))\n",
    "    print(word)\n",
    "    return docs[word]\n",
    "\n",
    "get_posting_list('ceramic', docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0bna0jzJ7YF"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پویا‌سازی نمایه (۷ + ۷ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "    برای پویا سازی نمایه ایجاد شده باید قابلیت حذف و اضافه تک داکیومنت اضافه شود .<br>\n",
    "    برای اضافه شدن داکیومنت، به تابع ()add_documnet یک سه‌تایی داده می‌شود که اطلاعات مربوط به داکیومنت شامل شناسه، عنوان و چکیده در آن به ترتیب قرار دارد. در صورت نبود آن سند در نمایه‌ها، به نمایه‌ها اضافه می‌شود.<br>\n",
    "     برای حذف داکیومنت نیز شناسه آن به تابع ()remove_document داده می‌شود.<br>\n",
    "    تضمین می‌شود که شرط یکتا بودن شناسه داکیومنت‌ها نقض نشود. برای مثال دو داکیومنت با شناسه یکسان به مجموعه اضافه نخواهد شد. البته ممکن است حذف شده و دوباره اضافه شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "ncB7fXTMJ7YG"
   },
   "outputs": [],
   "source": [
    "## 7 points\n",
    "\n",
    "def add_documnet(document : tuple):\n",
    "    \"\"\"Adds a document to positional index\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document : str\n",
    "        Comma separated string containing id,title,abstarct in this exact order\n",
    "    \"\"\"\n",
    "    add_doc_to_doc_dict(*document)\n",
    "    add_doc_to_index(paper_id_map[document[0]], docs)\n",
    "    return\n",
    "\n",
    "new_document = (\"1eae26fe1ca566f17468080c3aecab1c3f9efb66\", \n",
    "                \"A Deep Learning Framework for Viable Tumor Burden Estimation\", \n",
    "                \"Liver masses have become a common clinical challenge since they require to be defined and accurately categorized as neoplastic or nonneoplastic lesions. Hepatocellular carcinoma (HCC), the most common histologic type of primary liver malignancy, is a global health concern being the fifth most common cancer and the second cause of cancer mortality worldwide. Accurate diagnosis, which in some circumstances requires histopathology results, is necessary for appropriate management. Also, some tumor characteristics help in predicting tumor behavior and patient response to therapy. In this paper, we propose a deep learning framework for the segmentation of whole and viable tumor areas of liver cancer from whole-slide images (WSIs). To this end, we use Fast Segmentation Convolutional Neural Network (Fast-SCNN) as our network. We use the dataset from PAIP 2019 challenge. After data-augmentation on the training subset, we train the network with a multi-term loss function and SWA technique. Our model achieves 0.80 for the median of the Jaccard Index for the task of Viable Tumor Segmentation and 0.77 for the median of Weighted Absolute Accuracy for the task of Viable Tumor Burden Estimation on the whole-slide images of the test subset.\")\n",
    "add_documnet(new_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "BlnCr81fJ7YG"
   },
   "outputs": [],
   "source": [
    "## 7 points\n",
    "\n",
    "def remove_document(document_id : str):\n",
    "    \"\"\"removes a document from positional index\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document_id : str\n",
    "        Id of the document\n",
    "    \"\"\"\n",
    "    id = paper_id_map[document_id]\n",
    "    for term in doc_dict[id]['title'] + doc_dict[id]['abstract']:\n",
    "      if id in docs[term]['title'].keys():\n",
    "        del docs[term]['title'][id]\n",
    "      if id in docs[term]['abstract'].keys():\n",
    "        del docs[term]['abstract'][id]\n",
    "      if len(docs[term]['title'].keys()) == 0 and len(docs[term]['abstract'].keys()) == 0:\n",
    "        del docs[term]\n",
    "    del paper_id_map[document_id]\n",
    "    \n",
    "    return\n",
    "  \n",
    "remove_document(\"1eae26fe1ca566f17468080c3aecab1c3f9efb66\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfRWQpMBJ7YG"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>ذخیره و فشرده‌سازی نمایه (۱۳ + ۷\n",
    "     نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "    در این بخش باید توانایی ذخیره کردن نمایه و بارگذاری مجدد آن را به سامانه اضافه کنید.<br>\n",
    "    ذخیره‌سازی به ۳ روش صورت می‌گیرد.<br>\n",
    "     <ul>\n",
    "    <li>no-compression</li>\n",
    "    <li>gamma-code</li>\n",
    "    <li>variable-byte</li>\n",
    "    </ul> \n",
    "      روش‌های فشرده‌سازی باید توسط خودتان پیاده‌سازی شود.\n",
    "    برای ذخیره نمایه در فایل نیز از JSON  یا TXT استفاده کنید. \n",
    "    نام فایل خود را نوع فشرده‌سازی بگذارید و در یک فایل زیپ قرار دهید و آیلود کنید.\n",
    "    <br>\n",
    "     بخشی از نمره شما در این قسمت به میزان فشرده‌سازی نمایه اختصاص داده شده است. بنابراین پیاده‌سازی بهینه روش‌های فشرده‌سازی مهم است.<br>\n",
    "     تابع load_index برای بارگذاری نمایه است با گرفتن مسیر فایل ذخیره شده نمایه با نام path نمایه را از این فایل بارگذاری می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "6iETi2jHpoLM"
   },
   "outputs": [],
   "source": [
    "gaps_docs = {}\n",
    "for token in list(docs.keys()):\n",
    "  gaps_docs[token] = {\n",
    "      'title': {},\n",
    "      'abstract': {}\n",
    "  }\n",
    "  last = 0\n",
    "  for i in docs[token]['title'].keys():\n",
    "    gaps_docs[token]['title'][i-last] = docs[token]['title'][i]\n",
    "    last = i\n",
    "  last = 0\n",
    "  for i in docs[token]['abstract']:\n",
    "    gaps_docs[token]['abstract'][i-last] = docs[token]['abstract'][i]\n",
    "    last = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "UhRguICxzPO3"
   },
   "outputs": [],
   "source": [
    "from struct import pack, unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "id": "PS4wQ_5-kUHh"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def elias_gamma_encode(numbers):\n",
    "    encoded_string = ''\n",
    "    for num in numbers:\n",
    "        if num == 0:\n",
    "            encoded_string += '0'\n",
    "        else:\n",
    "            num_bits = int(math.log2(num)) + 1\n",
    "            unary_code = '1' * num_bits + '0'\n",
    "            binary_code = '{0:b}'.format(num)\n",
    "            binary_code = binary_code[1:]\n",
    "            encoded_string += unary_code + binary_code\n",
    "\n",
    "\n",
    "    encoded_bytes = int(encoded_string, 2).to_bytes((len(encoded_string) + 7) // 8, byteorder='big')\n",
    "    return encoded_bytes\n",
    "\n",
    "def elias_gamma_decode(encoded_bytes):\n",
    "    encoded_string = bin(int.from_bytes(encoded_bytes, byteorder='big'))[2:]\n",
    "    decoded_data = []\n",
    "    while encoded_string:\n",
    "        num_bits = encoded_string.index('0')\n",
    "        binary_code = '1' + encoded_string[num_bits + 1:num_bits*2]\n",
    "        encoded_string = encoded_string[num_bits*2:]\n",
    "        decoded_value = int(binary_code, 2)\n",
    "        decoded_data.append(decoded_value)\n",
    "\n",
    "    return decoded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "id": "X4Cfs6WSp7BA"
   },
   "outputs": [],
   "source": [
    "def vb_encode_number(number):\n",
    "    bl = []\n",
    "    while True:\n",
    "        r = number % 128\n",
    "        bl.insert(0, r)\n",
    "        if number < 128:\n",
    "            break\n",
    "        number = number // 128\n",
    "    bl[-1] += 128\n",
    "    return pack('%dB' % len(bl), *bl)\n",
    "\n",
    "def vb_encode(numbers):\n",
    "    bl = []\n",
    "    for number in numbers:\n",
    "        bl.append(vb_encode_number(number))\n",
    "    return b\"\".join(bl)\n",
    "\n",
    "def vb_decode(binary_string):\n",
    "    number = 0\n",
    "    numbers = []\n",
    "    binary_string = unpack('%dB' % len(binary_string), binary_string)\n",
    "    for byte in binary_string:\n",
    "        if byte < 128:\n",
    "            number = 128 * number + byte\n",
    "        else:\n",
    "            number = 128 * number + (byte - 128)\n",
    "            numbers.append(number)\n",
    "            number = 0\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "id": "e8B18dI8oc7p"
   },
   "outputs": [],
   "source": [
    "def compress(docs, f_encode):\n",
    "  vb_docs = {}\n",
    "  for token in list(docs.keys()):\n",
    "    vb_docs[token] = {\n",
    "        'title': {},\n",
    "        'abstract': {}\n",
    "    }\n",
    "    for i in docs[token]['title'].keys():\n",
    "      vb_docs[token]['title'][i] = f_encode(docs[token]['title'][i])\n",
    "    for i in docs[token]['abstract'].keys():\n",
    "      vb_docs[token]['abstract'][i] = f_encode(docs[token]['abstract'][i])\n",
    "  return vb_docs\n",
    "\n",
    "def decompress(vb_docs, f_decode):\n",
    "  docs = {}\n",
    "  for token in list(vb_docs.keys()):\n",
    "    docs[token] = {\n",
    "        'title': {},\n",
    "        'abstract': {}\n",
    "    }\n",
    "    for i in vb_docs[token]['title'].keys():\n",
    "      docs[token]['title'][i] = f_decode(vb_docs[token]['title'][i])\n",
    "    for i in vb_docs[token]['abstract'].keys():\n",
    "      docs[token]['abstract'][i] = f_decode(vb_docs[token]['abstract'][i])\n",
    "  return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "iC4-1JBqJ7YH"
   },
   "outputs": [],
   "source": [
    "## 13 points\n",
    "import pickle\n",
    "\n",
    "def store_index(path: str, compression_type: str):\n",
    "    \"\"\"Stores the index in a file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to store the file\n",
    "\n",
    "    compression_type : str\n",
    "        Could be one of the followings:\n",
    "        - no-compression\n",
    "        - gamma-code\n",
    "        - variable-byte\n",
    "\n",
    "    Returns\n",
    "    int\n",
    "        The size of the stored file\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if compression_type == 'no-compression':\n",
    "      with open(path, 'wb') as f:\n",
    "        # json.dump(docs, fp, cls=SetEncoder)\n",
    "            pickle.dump(gaps_docs, f)\n",
    "            return f.tell() / 10 ** 6\n",
    "    elif compression_type == 'gamma-code':\n",
    "      with open(path, 'wb') as f:\n",
    "          g_docs = compress(gaps_docs, elias_gamma_encode)\n",
    "          pickle.dump(g_docs, f)\n",
    "          return f.tell() / 10 ** 6\n",
    "    elif compression_type == 'variable-byte':\n",
    "      with open(path, 'wb') as f:\n",
    "          vb_docs = compress(gaps_docs, vb_encode)\n",
    "          pickle.dump(vb_docs, f)\n",
    "          return f.tell() / 10 ** 6\n",
    "\n",
    "    file_size = os.path.getsize(path) / 10**6\n",
    "    print(\"File Size is :\", file_size, \"MB\")\n",
    "    return file_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KdIDfVwv1Ubx",
    "outputId": "fc6b17e1-099f-4b27-f07d-a3c45995a6d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.476652"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_index(\"vb_index.txt\", \"variable-byte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yqwOQEjKrYFA",
    "outputId": "4b8a71f9-fe53-4c8b-9fd3-0ea4e9d252a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.661502"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_index(\"gamma_index.txt\", 'gamma-code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iOnacSj_6z2m",
    "outputId": "49496f12-4c22-4dbb-e29e-e7c302b05d04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.81669"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_index(\"index.txt\", \"no-compression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "Pgw9mSX4J7YH"
   },
   "outputs": [],
   "source": [
    "## 7 points\n",
    "\n",
    "def load_index(path: str, compression_type: str):\n",
    "    \"\"\"Loads the index from a file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path of the file to load from\n",
    "\n",
    "    compression_type : str\n",
    "        Could be one of the followings:\n",
    "        - no-compression\n",
    "        - gamma-code\n",
    "        - variable-byte\n",
    "    \"\"\"\n",
    "    if compression_type == 'no-compression':\n",
    "      with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    elif compression_type == 'gamma-code':\n",
    "      with open(path, \"rb\") as f:\n",
    "        loaded = pickle.load(f)\n",
    "        return decompress(loaded, elias_gamma_decode)\n",
    "    elif compression_type == 'variable-byte':\n",
    "      with open(path, \"rb\") as f:\n",
    "        loaded = pickle.load(f)\n",
    "        return decompress(loaded, vb_decode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "id": "yk7NpmBfxIMJ"
   },
   "outputs": [],
   "source": [
    "nc_decompressed = load_index(\"index.txt\", \"no-compression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "id": "1AnrYI5ofUby"
   },
   "outputs": [],
   "source": [
    "vb_decompressed = load_index(\"vb_index.txt\", \"variable-byte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "id": "Bv4cZeifrpB8"
   },
   "outputs": [],
   "source": [
    "gamma_decompressed = load_index(\"gamma_index.txt\", \"gamma-code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNG9pudTvrl4",
    "outputId": "996ff6a4-74c0-4ce7-865e-15b24af61fa8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc_decompressed == vb_decompressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lyJeb16r7CB",
    "outputId": "d984cf86-a0c3-42f7-8cb2-ebcba6f174ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_decompressed == nc_decompressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4NEwIWkJ7YH"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>اصلاح پرسمان (۳ + ۹\n",
    "     نمره)</b>\n",
    "    </h1>\n",
    "    در صورتی که پرسمان ورودی دارای غلط املایی باشد یا به عبارتی لغاتی از آن در لغت‌نامه موجود نباشد، لازم است که با جستجوی لغات احتمالی و اصلاح پرسمان به ادامه جستجو پرداخته شود.\n",
    "    <br>\n",
    "    <br>\n",
    "    برای اینکار ابتدا باید bigramهای لغت را به دست آورید، سپس با معیار jaccard بیست لغتی که بیشترین تعداد bigram مشترک با لغت مورد نظر را دارند بدست آورید. در آخر با معیار minimum edit distance لغت جایگزین را برای لغت مورد نظر پیدا کنید.\n",
    "    <br>\n",
    "    <br>\n",
    "    نیازی به ذخیره‌سازی و فشرده‌سازی نمایه bigram نیست. همچنین می‌توانید از کد آماده برای محاسبه edit distance استفاده کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "id": "x1x10DrmjU_U"
   },
   "outputs": [],
   "source": [
    "def get_bigrams(word: str):\n",
    "    return set(a + b for a, b in zip(word, word[1:]))\n",
    "\n",
    "def add_word_to_bigram_index(bigram_index, bigram_index_dictionary: set, word):\n",
    "    if word not in bigram_index_dictionary:\n",
    "        for bigram in get_bigrams(word):\n",
    "            bigram_index[bigram].append(word)\n",
    "        bigram_index_dictionary.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABX2I6-8n15J",
    "outputId": "36464c1e-40dd-4ecf-a57a-f9bb58b14462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textdistance in ./venv/lib/python3.8/site-packages (4.5.0)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "id": "pfYOeGoaJ7YI"
   },
   "outputs": [],
   "source": [
    "## 3 points\n",
    "\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from textdistance import hamming, mlipns\n",
    "from nltk.metrics.distance  import edit_distance\n",
    "\n",
    "def create_bigram_index(texts) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Creates a bigram index for the spell correction\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts: List[str]\n",
    "        The titles and abstracts of articles\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary of bigrams and their occurence\n",
    "    \"\"\"\n",
    "    tokens = set()\n",
    "    for t in texts:\n",
    "      for token in pre_clean(t):\n",
    "        tokens.add(token)\n",
    "    bigram_index = defaultdict(list)\n",
    "    bigram_index_dictionary = set()\n",
    "    for token in tokens:\n",
    "      add_word_to_bigram_index(bigram_index, bigram_index_dictionary, token)\n",
    "\n",
    "    return bigram_index, bigram_index_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "uXIZoQlfj-V1"
   },
   "outputs": [],
   "source": [
    "bigram_index, bigram_index_dictionary = create_bigram_index(list(df[df['anstract']!=-1]['anstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "w9OSPAAZkl_7"
   },
   "outputs": [],
   "source": [
    "JACARD_LIMIT = 0.3\n",
    "SIMILITY_LIMIT = 0.6\n",
    "\n",
    "def get_most_similar_words(token, bigram_index, limit):\n",
    "  bigrams = get_bigrams(token)\n",
    "  bigram_words_list = [bigram_index[bg] for bg in bigrams]\n",
    "  counter = Counter(chain.from_iterable(bigram_words_list))\n",
    "  similars = []\n",
    "  for word, sim in counter.items():\n",
    "    jacard = sim / (len(get_bigrams(word)) + len(bigrams) - sim)\n",
    "    if jacard > JACARD_LIMIT:\n",
    "      similars.append((word, jacard))\n",
    "  similars = similars[:limit]\n",
    "  best = max(((-edit_distance(token, word), jacard, word) for word, jacard in similars), default=None)\n",
    "  if best != None:\n",
    "    return best[2]\n",
    "  return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pe4cDgS0mG06",
    "outputId": "6ce17c08-18a9-41fa-8241-98ec92475dbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beautiful'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_similar_words('beautifal', bigram_index, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "id": "NmYPdfaPJ7YI"
   },
   "outputs": [],
   "source": [
    "## 9 points\n",
    "\n",
    "def correct_text(text: str, bigram_index, bigram_index_dictionary, similar_words_limit: int=20 ) -> str:\n",
    "    \"\"\"\n",
    "    Correct the give query text, if it is misspelled\n",
    "\n",
    "    Paramters\n",
    "    ---------\n",
    "    text: str\n",
    "        The query text\n",
    "    \n",
    "    Returns\n",
    "    str\n",
    "        The corrected form of the given text\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "\n",
    "    return ' '.join(\n",
    "        token if token in bigram_index_dictionary else get_most_similar_words(token, bigram_index, similar_words_limit)\n",
    "        for token in tokens\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xHk9pTzToEYP",
    "outputId": "c7cffb34-8313-4194-8da8-3e5004ab5fbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drug mechanism enrichment analysis improves'"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text('Drug mechannism enraichment analysis improes', bigram_index, bigram_index_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqYyF87OJ7YI"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>جستجو و بازیابی اسناد (۲۰ + ۵ نمره)</b>\n",
    "    </h1>\n",
    "در این بخش لازم است که شما پرسمانی را که از کاربر میگیرید، در مجموعه اسناد نمایه شده جست و جو کنید. توجه داشته باشید که جست و جویی که انجام میدهید هم باید در عنوان مقاله و هم در چکیده آن انجام شود. در نهایت، اسناد باید به ترتیب امتیاز نهاییشان برگردانده شوند. امتیاز نهایی هر سند نیز از جمع وزن دار امتیاز جست و جو در عنوان و جست و جو در چکیده مقاله به دست می آید.\n",
    "<br>\n",
    "حال به توضیح چگونگی جست و جو میپردازیم. حال به توضیح چگونگی جست و جو میپردازیم. در این قسمت بنا بر این است که دو روش جستجو را پیاده سازی کنید. روش اول جست و جو، جست و جو ترتیب دار در فضای برداری \n",
    "<b>tf-idf</b>\n",
    "به روش های <b>ltn-lnn</b> و <b>ltc-lnc</b> می باشد. \n",
    "روش دوم، جستجو بر اساس احتمال (روش Okapi BM25) است.\n",
    "<br>\n",
    "به طور کلی، شما به کمک وزنی که به عنوان ورودی به تابع خود میدهید، میتوانید امتیاز نهایی را مطابق زیر محاسبه کنید. توجه داشته باشید که این وزن عددی بین 0 و 1 است و بدیهی است که در صورت مثلا صفر بودن (در مثال زیر) تاثیر عنوان در جست و جوی شما حذف می شود\n",
    "<br>\n",
    "\n",
    "\n",
    "final score = weight * abstract_score + (1 - weight) * title_score\n",
    "\n",
    "\n",
    "در روش دوم، نیاز است تا روش بازیابی اطلاعات بر اساس احتمالات را پیاده سازی کنید که در اسلایدهای مربوط به این قسمت توضیح داده شده است.\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "در تابع search که مربوط به جست و جوی پرسمان کلی است، به عنوان ورودی پارامتر پرسمان (query)، روش محاسبه امتیاز(method)، تعداد اسنادی که باید برگردانده شود (n) را ورودی می گیرید. علاوه بر این، به کمک پارامتر ورودی mode مشخص خواهید کرد که آیا جست و جو در عنوان مقاله و چکیده آن به طور جدا انجام می شود یا خیر و به کمک ورودی where نیز مشخص می کنیم که اگر قرار بود جست و جوی پرسمان. تنها در یکی از آنها انجام شود، کدام است.\n",
    "\n",
    "پس از به دست آوردن لیست خروجی‌ها، در صورتی که ورودی\n",
    "print\n",
    "مقدار\n",
    "True\n",
    "داشت، لیست مقالات را به صورت خوانا و همراه با \n",
    "snippet\n",
    "در خروجی چاپ کنید (۵ نمره).\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "id": "jKAyFaNJMeh8"
   },
   "outputs": [],
   "source": [
    "def calculate_tf(token_list, term):\n",
    "  return 1 + np.log(token_list.count(term))\n",
    "\n",
    "def compute_tf(documents):\n",
    "  tf_dict = {}\n",
    "  for doc_id in documents.keys():\n",
    "    tf_dict[doc_id] = {\n",
    "        'title': {},\n",
    "        'abstract': {}\n",
    "    }\n",
    "    if documents[doc_id]['title']:\n",
    "      for term in documents[doc_id]['title']:\n",
    "        tf_dict[doc_id]['title'][term] = calculate_tf(documents[doc_id]['title'], term)\n",
    "    if documents[doc_id]['abstract']:\n",
    "      for term in documents[doc_id]['abstract']:\n",
    "        tf_dict[doc_id]['abstract'][term] = calculate_tf(documents[doc_id]['abstract'], term)\n",
    "  \n",
    "  return tf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_span(k, lst):\n",
    "    n = len(lst)\n",
    "    freq = [0] * (k + 1)\n",
    "    count = 0\n",
    "    left = 0\n",
    "    min_span = float('inf')\n",
    "\n",
    "    for right in range(n):\n",
    "        if freq[lst[right]] == 0:\n",
    "            count += 1\n",
    "        freq[lst[right]] += 1\n",
    "\n",
    "        while count == k:\n",
    "            min_span = min(min_span, right - left + 1)\n",
    "            freq[lst[left]] -= 1\n",
    "            if freq[lst[left]] == 0:\n",
    "                count -= 1\n",
    "            left += 1\n",
    "\n",
    "    return min_span\n",
    "\n",
    "def compute_proximity(query_tokens, doc_id, index):\n",
    "    term_list = []\n",
    "    for i, term in enumerate(query_tokens):\n",
    "        if index[term]['title']:\n",
    "            if doc_id in index[term]['title']:\n",
    "                for j in index[term]['title'][doc_id]:\n",
    "                    term_list.insert(j, i)\n",
    "            else:\n",
    "                return math.inf\n",
    "    return min_span(len(query_tokens), term_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "id": "B9Ds4iRePVzc"
   },
   "outputs": [],
   "source": [
    "tf_doc = compute_tf(doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "id": "rQPklL88P656"
   },
   "outputs": [],
   "source": [
    "def compute_idf(index):\n",
    "  N = len(paper_id_map.keys())\n",
    "  idf = {}\n",
    "  for term in index.keys():\n",
    "    idf[term] = {\n",
    "        'title':np.log(N / len(index[term]['title'].keys())) if len(index[term]['title'].keys()) > 0 else None,\n",
    "        'abstract':np.log(N / len(index[term]['abstract'].keys())) if len(index[term]['abstract'].keys()) > 0 else None\n",
    "    }\n",
    "  return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "id": "Q12f6XGwR20J"
   },
   "outputs": [],
   "source": [
    "idf_dict = compute_idf(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "id": "I7GBuc5gWa28"
   },
   "outputs": [],
   "source": [
    "def calculate_cosine(q_tfidf, d_tfidf, query_tokens, method):\n",
    "  dot_product = 0\n",
    "  qry_mod = 0\n",
    "  doc_mod = 0\n",
    "  for token in query_tokens:\n",
    "    qry_mod += q_tfidf[token] ** 2\n",
    "    doc_mod += d_tfidf[token] ** 2\n",
    "  qry_mod = np.sqrt(qry_mod)\n",
    "  doc_mod = np.sqrt(doc_mod)\n",
    "  if qry_mod * doc_mod == 0:\n",
    "    return 0\n",
    "  for token in query_tokens:\n",
    "    if method == 'n':\n",
    "      dot_product += q_tfidf[token] * d_tfidf[token]\n",
    "    else:\n",
    "      dot_product += (q_tfidf[token] / qry_mod) * (d_tfidf[token] / doc_mod)\n",
    "  \n",
    "  denominator = qry_mod * doc_mod\n",
    "  cos_sim = dot_product/denominator if method == 'n' else dot_product\n",
    "  return cos_sim\n",
    "\n",
    "def compute_query_tf_idf(query_tokens):\n",
    "  tf_idf = {}\n",
    "  for term in query_tokens:\n",
    "    tf = calculate_tf(query_tokens, term)\n",
    "    tf_idf[term] = tf\n",
    "  \n",
    "  return tf_idf\n",
    "\n",
    "def compute_similarity(title_query_tokens, abstract_query_token, weight, method):\n",
    "  tf_idf = {}\n",
    "  t_q_tf_idf = compute_query_tf_idf(title_query_tokens)\n",
    "  a_q_tf_idf = compute_query_tf_idf(abstract_query_token)\n",
    "  sim_list = []\n",
    "  for doc_id in tf_doc.keys():\n",
    "    tf_idf[doc_id] = {\n",
    "        'title': {},\n",
    "        'abstract': {}\n",
    "    }\n",
    "    for token in title_query_tokens:\n",
    "      tfidf_title = tf_doc[doc_id]['title'][token] * idf_dict[token]['title'] if (token in tf_doc[doc_id]['title'] and idf_dict[token]['title']) else 0\n",
    "      tf_idf[doc_id]['title'][token] = tfidf_title\n",
    "    for token in abstract_query_token:\n",
    "      tfidf_abstract = tf_doc[doc_id]['abstract'][token] * idf_dict[token]['abstract'] if (token in tf_doc[doc_id]['abstract'] and idf_dict[token]['abstract']) else 0\n",
    "      tf_idf[doc_id]['abstract'][token] = tfidf_abstract\n",
    "    cos_sim =  weight * calculate_cosine(t_q_tf_idf, tf_idf[doc_id]['title'], title_query_tokens, method) + (1.0 - weight) * calculate_cosine(a_q_tf_idf, tf_idf[doc_id]['abstract'], abstract_query_token, method)\n",
    "    # proximity = compute_proximity(query_tokens=title_query_tokens, doc_id=doc_id, index=docs)\n",
    "    # proximity_factor = 1 / (1 + 10 * proximity) \n",
    "    sim_list.append((cos_sim, doc_id))\n",
    "  sim_list.sort(key=lambda tup: tup[0],reverse=True)\n",
    "  return [x[1] for x in sim_list]\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "id": "_RCdrMHj-Dx3"
   },
   "outputs": [],
   "source": [
    "def compute_idf_okapi(index):\n",
    "  N = len(paper_id_map.keys())\n",
    "  idf = {}\n",
    "  for term in index.keys():\n",
    "    t_freq = len(index[term]['title'].keys())\n",
    "    a_freq = len(index[term]['abstract'].keys())\n",
    "    idf[term] = {\n",
    "        'title':np.log(1+ (N - t_freq + 0.5) / (t_freq + 0.5)),\n",
    "        'abstract': np.log(1+ (N - a_freq + 0.5) / (a_freq + 0.5)),\n",
    "        't_freq': t_freq,\n",
    "        'a_freq': a_freq\n",
    "    }\n",
    "  return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "id": "VC4QfjS7-wSB"
   },
   "outputs": [],
   "source": [
    "okapi_idf = compute_idf_okapi(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "id": "I-NGxqB6oeBs"
   },
   "outputs": [],
   "source": [
    "def compute_okapi_d_q(query_tokens, doc_id, avg_doc_len, k=1.5, b=0.75):\n",
    "  t_doc_len = len(doc_dict[doc_id]['title'])\n",
    "  a_doc_len = len(doc_dict[doc_id].get('abstract')) if doc_dict[doc_id].get('abstract') else 0\n",
    "  score = 0.0\n",
    "  for term in query_tokens:\n",
    "    if term in tf_doc[doc_id]['title'].keys():\n",
    "      freq = okapi_idf[term]['t_freq']\n",
    "      idf = okapi_idf[term]['title'] if  okapi_idf[term]['title'] else 0\n",
    "      numerator = idf * freq * (k + 1)\n",
    "      denominator = freq + k * (1 - b + b * t_doc_len / avg_doc_len)\n",
    "      score += (numerator / denominator)\n",
    "    if term in tf_doc[doc_id]['abstract'].keys():\n",
    "      freq = okapi_idf[term]['a_freq']\n",
    "      idf = okapi_idf[term]['abstract'] if  okapi_idf[term]['abstract'] else 0\n",
    "      numerator = idf * freq * (k + 1)\n",
    "      denominator = freq + k * (1 - b + b * a_doc_len / avg_doc_len)\n",
    "      score += (numerator / denominator)\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "id": "LQBw6loH5Yjy"
   },
   "outputs": [],
   "source": [
    "def compute_okapi(title_query_tokens, abstract_quesry_token, weight):\n",
    "  results = []\n",
    "  avg_title_len = sum([len(doc_dict[x]['title']) for x in doc_dict.keys()]) / len(doc_dict.keys())\n",
    "  for doc_id in doc_dict.keys():\n",
    "    score = compute_okapi_d_q(title_query_tokens, doc_id, avg_title_len)\n",
    "    results.append((score, doc_id))\n",
    "  results.sort(key=lambda tup: tup[0],reverse=True)\n",
    "  return [x[1] for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "id": "F6H8U_tLJ7YJ"
   },
   "outputs": [],
   "source": [
    "## 25 points\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "def print_docs(doc_ids, df):\n",
    "  paper_ids = []\n",
    "  for id in doc_ids:\n",
    "    paper_ids.append(list(paper_id_map.keys())[list(paper_id_map.values()).index(id)])\n",
    "  print(df[df.paperId.isin(paper_ids)].to_markdown())\n",
    "\n",
    "def search(title_query: str, abstract_query: str, max_result_count: int = 10, method: str = 'ltn-lnn', weight: float = 0.5, show=False):\n",
    "    \"\"\"\n",
    "        Finds relevant documents to query\n",
    "        \n",
    "        Parameters\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        max_result_count: Return top 'max_result_count' docs which have the highest scores. \n",
    "                          notice that if max_result_count = -1, then you have to return all docs\n",
    "        \n",
    "        mode: 'detailed' for searching in title and text separately.\n",
    "              'overall' for all words, and weighted by where the word apears on.\n",
    "        \n",
    "        where: when mode ='detailed', when we want search query \n",
    "                in title or text not both of them at the same time.\n",
    "        \n",
    "        method: 'ltn-lnn' or 'ltc-lnc' or 'okapi25'\n",
    "\n",
    "        Returns\n",
    "        ----------------------------------------------------------------------------------------------------\n",
    "        list\n",
    "        Retreived documents with snippet\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('data/data.csv')\n",
    "    t_q_tokens = [x for x in clean_data(title_query) if x not in stop_words_list]\n",
    "    a_q_tokens = [x for x in clean_data(abstract_query) if x not in stop_words_list]\n",
    "    if method == 'okapi25':\n",
    "      result = compute_okapi(t_q_tokens, a_q_tokens, weight)\n",
    "    else:\n",
    "      result = compute_similarity(t_q_tokens, a_q_tokens, weight, method[-1])\n",
    "\n",
    "    if show:\n",
    "       doc_ids = result[:max_result_count]\n",
    "       print_docs(doc_ids, df)\n",
    "    return result[:max_result_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in ./venv/lib/python3.8/site-packages (0.9.0)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|      | paperId                                  | title                                                                                                 | anstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |   Unnamed: 3 |   Unnamed: 4 |\n",
      "|-----:|:-----------------------------------------|:------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------:|-------------:|\n",
      "|   16 | b9b4e05faa194e5022edd9eb9dd07e3d675c2b36 | Feature Pyramid Networks for Object Detection                                                         | Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |          nan |          nan |\n",
      "|  464 | e2751a898867ce6687e08a5cc7bdb562e999b841 | FCOS: Fully Convolutional One-Stage Object Detection                                                  | We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor-box free, as well as proposal free. By eliminating the pre-defined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training and significantly reduces the training memory footprint. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), our detector FCOS outperforms previous anchor-based one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at: this https URL                                                                                                                                                                                                                                                                                                                                                                                                                                     |          nan |          nan |\n",
      "|  580 | 55e0b7cae68a817d91756d02c7eb04a7079d7b5a | Salient Object Detection with Recurrent Fully Convolutional Networks                                  | Deep networks have been proved to encode high-level features with semantic meaning and delivered superior performance in salient object detection. In this paper, we take one step further by developing a new saliency detection method based on recurrent fully convolutional networks (RFCNs). Compared with existing deep network based methods, the proposed network is able to incorpor- ate saliency prior knowledge for more accurate inference. In addition, the recurrent architecture enables our method to automatically learn to refine the saliency map by iteratively correcting its previous errors, yielding more reliable final predictions. To train such a netw- ork with numerous parameters, we propose a pre-training strategy using semantic segmentation data, which simultaneously leverages the strong supervision of segmentation tasks for effective training and enables the network to capture generic representations to chara- cterize category-agnostic objects for saliency detection. Extensive experimental evaluations demonstrate that the proposed method compares favorably against state-of-the-art saliency detection approaches. Additional validations are also performed to study the impact of the recurrent architecture and pre-training strategy on both saliency detection and semantic segmentation, which provides important knowledge for network design and training in the future research.                                                                                                                                                                                                                                           |          nan |          nan |\n",
      "|  609 | 20114810f64a7ca05b3b9acbc873a74d9de9c13a | Unifying Voxel-based Representation with Transformer for 3D Object Detection                          | In this work, we present a uniﬁed framework for multi-modality 3D object detection, named UVTR. The proposed method aims to unify multi-modality representations in the voxel space for accurate and robust single- or cross-modality 3D detection. To this end, the modality-speciﬁc space is ﬁrst designed to represent different inputs in the voxel feature space. Different from previous work, our approach preserves the voxel space without height compression to alleviate semantic ambiguity and enable spatial connections. To make full use of the inputs from different sensors, the cross-modality interaction is then proposed, including knowledge transfer and modality fusion. In this way, geometry-aware expressions in point clouds and context-rich features in images are well utilized for better performance and robustness. The transformer decoder is applied to efﬁciently sample features from the uniﬁed space with learnable positions, which facilitates object-level interactions. In general, UVTR presents an early attempt to represent different modalities in a uniﬁed framework. It surpasses previous work in single- or multi-modality entries. The proposed method achieves leading performance in the nuScenes test set for both object detection and the following object tracking task. Code is made publicly available at https://github.com/dvlab-research/UVTR.                                                                                                                                                                                                                                                                               |          nan |          nan |\n",
      "|  813 | a7c3bdca5b1c2ca860a4d24d0b35f86b056bb30d | WSOD2: Learning Bottom-Up and Top-Down Objectness Distillation for Weakly-Supervised Object Detection | We study on weakly-supervised object detection (WSOD) which plays a vital role in relieving human involvement from object-level annotations. Predominant works integrate region proposal mechanisms with convolutional neural networks (CNN). Although CNN is proficient in extracting discriminative local features, grand challenges still exist to measure the likelihood of a bounding box containing a complete object (i.e., “objectness”). In this paper, we propose a novel WSOD framework with Objectness Distillation (i.e., WSOD2) by designing a tailored training mechanism for weakly-supervised object detection. Multiple regression targets are specifically determined by jointly considering bottom-up (BU) and top-down (TD) objectness from low-level measurement and CNN confidences with an adaptive linear combination. As bounding box regression can facilitate a region proposal learning to approach its regression target with high objectness during training, deep objectness representation learned from bottom-up evidences can be gradually distilled into CNN by optimization. We explore different adaptive training curves for BU/TD objectness, and show that the proposed WSOD2 can achieve state-of-the-art results.                                                                                                                                                                                                                                                                                                                                                                                                                                  |          nan |          nan |\n",
      "| 1217 | b0204e6c7301281ec8460e5251cd7c3c83c40883 | Pano-RSOD: A Dataset and Benchmark for Panoramic Road Scene Object Detection                          | Panoramic images have a wide range of applications in many fields with their ability to perceive all-round information. Object detection based on panoramic images has certain advantages in terms of environment perception due to the characteristics of panoramic images, e.g., lager perspective. In recent years, deep learning methods have achieved remarkable results in image classification and object detection. Their performance depends on the large amount of training data. Therefore, a good training dataset is a prerequisite for the methods to achieve better recognition results. Then, we construct a benchmark named Pano-RSOD for panoramic road scene object detection. Pano-RSOD contains vehicles, pedestrians, traffic signs and guiding arrows. The objects of Pano-RSOD are labelled by bounding boxes in the images. Different from traditional object detection datasets, Pano-RSOD contains more objects in a panoramic image, and the high-resolution images have 360-degree environmental perception, more annotations, more small objects and diverse road scenes. The state-of-the-art deep learning algorithms are trained on Pano-RSOD for object detection, which demonstrates that Pano-RSOD is a useful benchmark, and it provides a better panoramic image training dataset for object detection tasks, especially for small and deformed objects.                                                                                                                                                                                                                                                                                                |          nan |          nan |\n",
      "| 1330 | 117ac7c5d42bfdbe83432672db05a2de08dae113 | A Mutual Learning Method for Salient Object Detection With Intertwined Multi-Supervision              | Though deep learning techniques have made great progress in salient object detection recently, the predicted saliency maps still suffer from incomplete predictions due to the internal complexity of objects and inaccurate boundaries caused by strides in convolution and pooling operations. To alleviate these issues, we propose to train saliency detection networks by exploiting the supervision from not only salient object detection, but also foreground contour detection and edge detection. First, we leverage salient object detection and foreground contour detection tasks in an intertwined manner to generate saliency maps with uniform highlight. Second, the foreground contour and edge detection tasks guide each other simultaneously, thereby leading to preciser foreground contour prediction and reducing the local noises for edge prediction. In addition, we develop a novel mutual learning module (MLM) which serves as the building block of our method. Each MLM consists of multiple network branches trained in a mutual learning manner, which improves the performance by a large margin. Extensive experiments on seven challenging datasets demonstrate that the proposed method has delivered state-of-the-art results in both salient object detection and edge detection.                                                                                                                                                                                                                                                                                                                                                                     |          nan |          nan |\n",
      "| 1590 | 04957e40d47ca89d38653e97f728883c0ad26e5d | Cascade R-CNN: Delving Into High Quality Object Detection                                             | In object detection, an intersection over union (IoU) threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code is available at https://github.com/zhaoweicai/cascade-rcnn. |          nan |          nan |\n",
      "| 1660 | 72564a69bf339ff1d16a639c86a764db2321caab | Focal Loss for Dense Object Detection                                                                 | The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.                                                                                                                                                                                                                                                                                                                                                         |          nan |          nan |\n",
      "| 1715 | 2f4df08d9072fc2ac181b7fced6a245315ce05c8 | Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation                      | Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |          nan |          nan |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[17, 465, 581, 610, 814, 1218, 1331, 1591, 1661, 1716]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(title_query=\"object detection\", abstract_query=\"object dection\", weight=0.5, show=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nr5FX6ZQJ7YJ"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>\n",
    "     ارزیابی عملکرد سامانه (۷ + ۱۵\n",
    "     نمره)   \n",
    "     </b>\n",
    "    </h1>\n",
    "    در این قسمت تعدادی پرسمان نمونه به همراه اسناد متناظر برای آن‌ها در فایل validation در اختیار شما قرار گرفته است. در هر مورد اطلاعات لازم برای ایجاد یک پرسمان ذکر شده است. مطابق آن‌ها پرسمان خود را ایجاد کنید سپس نتایج به دست آمده از هر پرسمان را به عنوان predicted results آن پرسمان در نظر بگیرید. همچنین در هر مورد لیستی از شناسه‌ها وجود دارد. این لیست را به عنوان actual results در نظر بگیرید.\n",
    "    <br>\n",
    "    <br>\n",
    "    معیار‌های زیر را پیاده‌سازی کنید (بدون استفاده از کد آماده) و نتیجه این معیارها را روی مجموعه‌ی actual و predicted گزارش کنید. دقت کنید که به ازای هر پرسمان و همچنین مجموع پرسمان‌ها باید تمام معیارها را در قالب مشخص شده گزارش کنید.\n",
    "    <br>\n",
    "    <br>\n",
    "    در این قسمت ۷ نمره به پیاده‌سازی صحیح و ۱۵ نمره به کیفیت نهایی بازیابی مربوط می‌شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "id": "1vqR0EDHHv4K"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/validation.json') as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "for q in d.keys():\n",
    "  l = []\n",
    "  for doc_id in d[q]:\n",
    "    id = paper_id_map[doc_id]\n",
    "    l.append(id)\n",
    "    d[q] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "id": "CTb5dpmqKfla"
   },
   "outputs": [],
   "source": [
    "methods = ['ltn-lnn','ltc-lnc','okapi25']\n",
    "results = {}\n",
    "for query in d.keys():\n",
    "  results[query] = {}\n",
    "  for method in methods:\n",
    "    results[query][method] = search(query, query,method=method, weight=0.5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "id": "QjUMpT90J7YJ"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def calculate_precision(actual: List[List[str]], predicted: List[List[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the precision of the predicted results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : List[List[str]]\n",
    "        The actual results\n",
    "    predicted : List[List[str]]\n",
    "        The predicted results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The precision of the predicted results\n",
    "    \"\"\" \n",
    "    tps = np.sum(np.array([len([x for x in predicted[i] if x in actual[i]]) for i in range(len(predicted))]))\n",
    "    fps = np.sum(np.array([len([x for x in predicted[i] if x not in actual[i]]) for i in range(len(predicted))]))\n",
    "    # print(tps, fps)\n",
    "    return tps / (tps + fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "id": "EEZG_7SZJ7YK"
   },
   "outputs": [],
   "source": [
    "def calculate_recall(actual: List[List[str]], predicted: List[List[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the recall of the predicted results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : List[List[str]]\n",
    "        The actual results\n",
    "    predicted : List[List[str]]\n",
    "        The predicted results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The recall of the predicted results\n",
    "    \"\"\"\n",
    "    tps = np.sum(np.array([len([x for x in predicted[i] if x in actual[i]]) for i in range(len(predicted))]))\n",
    "    fns = np.sum(np.array([len([x for x in actual[i] if x not in predicted[i]]) for i in range(len(predicted))]))\n",
    "    return tps / (tps + fns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "id": "dXftCHTmJ7YK"
   },
   "outputs": [],
   "source": [
    "def calculate_F1(actual: List[List[str]], predicted: List[List[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the F1 score of the predicted results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : List[List[str]]\n",
    "        The actual results\n",
    "    predicted : List[List[str]]\n",
    "        The predicted results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The F1 score of the predicted results    \n",
    "    \"\"\"\n",
    "    p = calculate_precision(actual, predicted)\n",
    "    r = calculate_recall(actual, predicted)\n",
    "\n",
    "    # TODO: Calculate F1 here\n",
    "\n",
    "    return (2 * r * p) / (r + p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "id": "9aaw7Q9DJ7YK"
   },
   "outputs": [],
   "source": [
    "def average_precision(actual, predicted):\n",
    "    score = 0\n",
    "    c = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] in actual:\n",
    "            c += 1\n",
    "            score += c / (i + 1)\n",
    "    \n",
    "    return score / c\n",
    "\n",
    "def calculate_MAP(actual: List[List[str]], predicted: List[List[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Mean Average Precision of the predicted results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : List[List[str]]\n",
    "        The actual results\n",
    "    predicted : List[List[str]]\n",
    "        The predicted results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The Mean Average Precision of the predicted results\n",
    "    \"\"\"\n",
    "    map = 0.0\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        ap = average_precision(actual=actual[i], predicted=predicted[i])\n",
    "        map += ap\n",
    "\n",
    "    return map / len(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "id": "MCA25wsRJ7YK"
   },
   "outputs": [],
   "source": [
    "def get_ndcg(predicted, actual):\n",
    "    dcg = []\n",
    "    for i, p in enumerate(predicted):\n",
    "        r = 10 - actual.index(p) if p in actual else 0\n",
    "        dcg.append(r / np.log2(i + 2))\n",
    "    return np.average(np.array(dcg)) \n",
    "\n",
    "def cacluate_NDCG(actual: List[List[str]], predicted: List[List[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Normalized Discounted Cumulative Gain (NDCG) of the predicted results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : List[List[str]]\n",
    "        The actual results\n",
    "    predicted : List[List[str]]\n",
    "        The predicted results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The NDCG of the predicted results\n",
    "    \"\"\"\n",
    "    r = list(range(1, 11))\n",
    "    perfect_dcg = np.average(np.array(r[::-1] / np.log2(np.arange(2, len(r) + 2))))\n",
    "    ndcg = []\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        dcg = get_ndcg(actual=actual[i], predicted=predicted[i])\n",
    "        ndcg.append(dcg)\n",
    "\n",
    "    return np.average(np.array(ndcg)/ perfect_dcg) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "id": "CakQSXVtJ7YK"
   },
   "outputs": [],
   "source": [
    "def reciprocal_rank(actual, predicted):\n",
    "    for i , p in enumerate(predicted):\n",
    "        if p in actual:\n",
    "            return 1 / (i + 1)\n",
    "\n",
    "def cacluate_MRR(actual: List[List[str]], predicted: List[List[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Mean Reciprocal Rank of the predicted results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : List[List[str]]\n",
    "        The actual results\n",
    "    predicted : List[List[str]]\n",
    "        The predicted results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The MRR of the predicted results\n",
    "    \"\"\"\n",
    "    MRR = 0.0\n",
    "\n",
    "    for a, p in zip(actual, predicted):\n",
    "        MRR += reciprocal_rank(a, p)\n",
    "\n",
    "    return MRR / len(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dhr0LRcnJ7YL",
    "outputId": "d6ea1254-0c9b-447c-cc5d-7c27a82b11e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ltn.lnn precision = 0.6833333333333333\n",
      "ltn.lnn recall = 0.6833333333333333\n",
      "ltn.lnn F1 = 0.6833333333333333\n",
      "ltn.lnn MAP = 0.8596891534391533\n",
      "ltn.lnn NDCG = 0.617947272632772\n",
      "ltn.lnn MRR = 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "# TODO: Call evaluation functions here and report the results for ltn.lnn\n",
    "actuals = [d[x] for x in d.keys()]\n",
    "predicted = [results[x]['ltn-lnn'] for x in d.keys()]\n",
    "print(f\"ltn.lnn precision = {calculate_precision(actuals, predicted)}\")\n",
    "print(f\"ltn.lnn recall = {calculate_recall(actuals, predicted)}\")\n",
    "print(f\"ltn.lnn F1 = {calculate_F1(actuals, predicted)}\")\n",
    "print(f\"ltn.lnn MAP = {calculate_MAP(actuals, predicted)}\")\n",
    "print(f\"ltn.lnn NDCG = {cacluate_NDCG(actuals, predicted)}\")\n",
    "print(f\"ltn.lnn MRR = {cacluate_MRR(actuals, predicted)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNutDUMaJ7YL",
    "outputId": "6cdda608-d31f-4cc0-c2a0-2103ffc4bb26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ltc.lnc precision = 0.6833333333333333\n",
      "ltc.lnc recall = 0.6833333333333333\n",
      "ltc.lnc F1 = 0.6833333333333333\n",
      "ltc.lnc MAP = 0.8596891534391533\n",
      "ltc.lnc NDCG = 0.6176928312616365\n",
      "ltc.lnc MRR = 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "# TODO: Call evaluation functions here and report the results for ltc.lnc\n",
    "actuals = [d[x] for x in d.keys()]\n",
    "predicted = [results[x]['ltc-lnc'] for x in d.keys()]\n",
    "print(f\"ltc.lnc precision = {calculate_precision(actuals, predicted)}\")\n",
    "print(f\"ltc.lnc recall = {calculate_recall(actuals, predicted)}\")\n",
    "print(f\"ltc.lnc F1 = {calculate_F1(actuals, predicted)}\")\n",
    "print(f\"ltc.lnc MAP = {calculate_MAP(actuals, predicted)}\")\n",
    "print(f\"ltc.lnc NDCG = {cacluate_NDCG(actuals, predicted)}\")\n",
    "print(f\"ltc.lnc MRR = {cacluate_MRR(actuals, predicted)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwtYQNCAJ7YL",
    "outputId": "873ddc9e-15cb-4258-f536-6ecc742db6ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okapi BM25 precision = 0.7\n",
      "Okapi BM25 recall = 0.7\n",
      "Okapi BM25 F1 = 0.7\n",
      "Okapi BM25 MAP = 0.7898840755437978\n",
      "Okapi BM25 NDCG = 0.6231708417640714\n",
      "Okapi BM25 MRR = 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "# TODO: Call evaluation functions here and report the results for Okapi BM25\n",
    "actuals = [d[x] for x in d.keys()]\n",
    "predicted = [results[x]['okapi25'] for x in d.keys()]\n",
    "print(f\"Okapi BM25 precision = {calculate_precision(actuals, predicted)}\")\n",
    "print(f\"Okapi BM25 recall = {calculate_recall(actuals, predicted)}\")\n",
    "print(f\"Okapi BM25 F1 = {calculate_F1(actuals, predicted)}\")\n",
    "print(f\"Okapi BM25 MAP = {calculate_MAP(actuals, predicted)}\")\n",
    "print(f\"Okapi BM25 NDCG = {cacluate_NDCG(actuals, predicted)}\")\n",
    "print(f\"Okapi BM25 MRR = {cacluate_MRR(actuals, predicted)}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08ac30a6a1fd2e576b33e03f7d61c3a285d7ee0582c2dd23dde6343ef303ebe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
